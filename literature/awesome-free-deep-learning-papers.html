<!DOCTYPE html>
<html>

<head>

<meta charset="utf-8">
<title>README</title>


<style type="text/css">
* {
  box-sizing: border-box; }

body {
  padding: 0;
  margin: 40px 70px;
  font-family: "Open Sans", "Helvetica Neue", Helvetica, Arial, sans-serif;
  font-size: 16px;
  line-height: 1.5;
  color: #606c71; }

a {
  color: #1e6bb8;
  text-decoration: none; }
  a:hover {
    text-decoration: underline; }

.btn {
  display: inline-block;
  margin-bottom: 1rem;
  color: rgba(255, 255, 255, 0.7);
  background-color: rgba(255, 255, 255, 0.08);
  border-color: rgba(255, 255, 255, 0.2);
  border-style: solid;
  border-width: 1px;
  border-radius: 0.3rem;
  transition: color 0.2s, background-color 0.2s, border-color 0.2s; }
  .btn:hover {
    color: rgba(255, 255, 255, 0.8);
    text-decoration: none;
    background-color: rgba(255, 255, 255, 0.2);
    border-color: rgba(255, 255, 255, 0.3); }
  .btn + .btn {
    margin-left: 1rem; }
  @media screen and (min-width: 64em) {
    .btn {
      padding: 0.75rem 1rem; } }
  @media screen and (min-width: 42em) and (max-width: 64em) {
    .btn {
      padding: 0.6rem 0.9rem;
      font-size: 0.9rem; } }
  @media screen and (max-width: 42em) {
    .btn {
      display: block;
      width: 100%;
      padding: 0.75rem;
      font-size: 0.9rem; }
      .btn + .btn {
        margin-top: 1rem;
        margin-left: 0; } }

.page-header {
  color: #fff;
  text-align: center;
  background-color: #159957;
  background-image: linear-gradient(120deg, #155799, #159957); }
  @media screen and (min-width: 64em) {
    .page-header {
      padding: 5rem 6rem; } }
  @media screen and (min-width: 42em) and (max-width: 64em) {
    .page-header {
      padding: 3rem 4rem; } }
  @media screen and (max-width: 42em) {
    .page-header {
      padding: 2rem 1rem; } }

.project-name {
  margin-top: 0;
  margin-bottom: 0.1rem; }
  @media screen and (min-width: 64em) {
    .project-name {
      font-size: 3.25rem; } }
  @media screen and (min-width: 42em) and (max-width: 64em) {
    .project-name {
      font-size: 2.25rem; } }
  @media screen and (max-width: 42em) {
    .project-name {
      font-size: 1.75rem; } }

.project-tagline {
  margin-bottom: 2rem;
  font-weight: normal;
  opacity: 0.7; }
  @media screen and (min-width: 64em) {
    .project-tagline {
      font-size: 1.25rem; } }
  @media screen and (min-width: 42em) and (max-width: 64em) {
    .project-tagline {
      font-size: 1.15rem; } }
  @media screen and (max-width: 42em) {
    .project-tagline {
      font-size: 1rem; } }

.main-content {
  word-wrap: break-word; }
  .main-content :first-child {
    margin-top: 0; }
  @media screen and (min-width: 64em) {
    .main-content {
      max-width: 64rem;
      padding: 2rem 6rem;
      margin: 0 auto;
      font-size: 1.1rem; } }
  @media screen and (min-width: 42em) and (max-width: 64em) {
    .main-content {
      padding: 2rem 4rem;
      font-size: 1.1rem; } }
  @media screen and (max-width: 42em) {
    .main-content {
      padding: 2rem 1rem;
      font-size: 1rem; } }
  .main-content img {
    max-width: 100%; }
  .main-content h1,
  .main-content h2,
  .main-content h3,
  .main-content h4,
  .main-content h5,
  .main-content h6 {
    margin-top: 2rem;
    margin-bottom: 1rem;
    font-weight: normal;
    color: #159957; }
  .main-content p {
    margin-bottom: 1em; }
  .main-content code {
    padding: 2px 4px;
    font-family: Consolas, "Liberation Mono", Menlo, Courier, monospace;
    font-size: 0.9rem;
    color: #567482;
    background-color: #f3f6fa;
    border-radius: 0.3rem; }
  .main-content pre {
    padding: 0.8rem;
    margin-top: 0;
    margin-bottom: 1rem;
    font: 1rem Consolas, "Liberation Mono", Menlo, Courier, monospace;
    color: #567482;
    word-wrap: normal;
    background-color: #f3f6fa;
    border: solid 1px #dce6f0;
    border-radius: 0.3rem; }
    .main-content pre > code {
      padding: 0;
      margin: 0;
      font-size: 0.9rem;
      color: #567482;
      word-break: normal;
      white-space: pre;
      background: transparent;
      border: 0; }
  .main-content .highlight {
    margin-bottom: 1rem; }
    .main-content .highlight pre {
      margin-bottom: 0;
      word-break: normal; }
  .main-content .highlight pre,
  .main-content pre {
    padding: 0.8rem;
    overflow: auto;
    font-size: 0.9rem;
    line-height: 1.45;
    border-radius: 0.3rem;
    -webkit-overflow-scrolling: touch; }
  .main-content pre code,
  .main-content pre tt {
    display: inline;
    max-width: initial;
    padding: 0;
    margin: 0;
    overflow: initial;
    line-height: inherit;
    word-wrap: normal;
    background-color: transparent;
    border: 0; }
    .main-content pre code:before, .main-content pre code:after,
    .main-content pre tt:before,
    .main-content pre tt:after {
      content: normal; }
  .main-content ul,
  .main-content ol {
    margin-top: 0; }
  .main-content blockquote {
    padding: 0 1rem;
    margin-left: 0;
    color: #819198;
    border-left: 0.3rem solid #dce6f0; }
    .main-content blockquote > :first-child {
      margin-top: 0; }
    .main-content blockquote > :last-child {
      margin-bottom: 0; }
  .main-content table {
    display: block;
    width: 100%;
    overflow: auto;
    word-break: normal;
    word-break: keep-all;
    -webkit-overflow-scrolling: touch; }
    .main-content table th {
      font-weight: bold; }
    .main-content table th,
    .main-content table td {
      padding: 0.5rem 1rem;
      border: 1px solid #e9ebec; }
  .main-content dl {
    padding: 0; }
    .main-content dl dt {
      padding: 0;
      margin-top: 1rem;
      font-size: 1rem;
      font-weight: bold; }
    .main-content dl dd {
      padding: 0;
      margin-bottom: 1rem; }
  .main-content hr {
    height: 2px;
    padding: 0;
    margin: 1rem 0;
    background-color: #eff0f1;
    border: 0; }

.site-footer {
  padding-top: 2rem;
  margin-top: 2rem;
  border-top: solid 1px #eff0f1; }
  @media screen and (min-width: 64em) {
    .site-footer {
      font-size: 1rem; } }
  @media screen and (min-width: 42em) and (max-width: 64em) {
    .site-footer {
      font-size: 1rem; } }
  @media screen and (max-width: 42em) {
    .site-footer {
      font-size: 0.9rem; } }

.site-footer-owner {
  display: block;
  font-weight: bold; }

.site-footer-credits {
  color: #819198; }
</style>

<style type="text/css">
/*
 Solarized Color Schemes originally by Ethan Schoonover
 http://ethanschoonover.com/solarized

 Ported for PrismJS by Hector Matos
 Website: https://krakendev.io
 Twitter Handle: https://twitter.com/allonsykraken)
*/

/*
SOLARIZED HEX
--------- -------
base03    #002b36
base02    #073642
base01    #586e75
base00    #657b83
base0     #839496
base1     #93a1a1
base2     #eee8d5
base3     #fdf6e3
yellow    #b58900
orange    #cb4b16
red       #dc322f
magenta   #d33682
violet    #6c71c4
blue      #268bd2
cyan      #2aa198
green     #859900
*/

code[class*="language-"],
pre[class*="language-"] {
	color: #657b83; /* base00 */
	font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
	text-align: left;
	white-space: pre;
	word-spacing: normal;
	word-break: normal;
	word-wrap: normal;

	line-height: 1.5;

	-moz-tab-size: 4;
	-o-tab-size: 4;
	tab-size: 4;

	-webkit-hyphens: none;
	-moz-hyphens: none;
	-ms-hyphens: none;
	hyphens: none;
}

pre[class*="language-"]::-moz-selection, pre[class*="language-"] ::-moz-selection,
code[class*="language-"]::-moz-selection, code[class*="language-"] ::-moz-selection {
	background: #073642; /* base02 */
}

pre[class*="language-"]::selection, pre[class*="language-"] ::selection,
code[class*="language-"]::selection, code[class*="language-"] ::selection {
	background: #073642; /* base02 */
}

/* Code blocks */
pre[class*="language-"] {
	padding: 1em;
	margin: .5em 0;
	overflow: auto;
	border-radius: 0.3em;
}

:not(pre) > code[class*="language-"],
pre[class*="language-"] {
	background-color: #fdf6e3; /* base3 */
}

/* Inline code */
:not(pre) > code[class*="language-"] {
	padding: .1em;
	border-radius: .3em;
}

.token.comment,
.token.prolog,
.token.doctype,
.token.cdata {
	color: #93a1a1; /* base1 */
}

.token.punctuation {
	color: #586e75; /* base01 */
}

.namespace {
	opacity: .7;
}

.token.property,
.token.tag,
.token.boolean,
.token.number,
.token.constant,
.token.symbol,
.token.deleted {
	color: #268bd2; /* blue */
}

.token.selector,
.token.attr-name,
.token.string,
.token.char,
.token.builtin,
.token.url,
.token.inserted {
	color: #2aa198; /* cyan */
}

.token.entity {
	color: #657b83; /* base00 */
	background: #eee8d5; /* base2 */
}

.token.atrule,
.token.attr-value,
.token.keyword {
	color: #859900; /* green */
}

.token.function {
	color: #b58900; /* yellow */
}

.token.regex,
.token.important,
.token.variable {
	color: #cb4b16; /* orange */
}

.token.important,
.token.bold {
	font-weight: bold;
}
.token.italic {
	font-style: italic;
}

.token.entity {
	cursor: help;
}
</style>

<style type="text/css">
pre.line-numbers {
	position: relative;
	padding-left: 3.8em;
	counter-reset: linenumber;
}

pre.line-numbers > code {
	position: relative;
}

.line-numbers .line-numbers-rows {
	position: absolute;
	pointer-events: none;
	top: 0;
	font-size: 100%;
	left: -3.8em;
	width: 3em; /* works for line-numbers below 1000 lines */
	letter-spacing: -1px;
	border-right: 1px solid #999;

	-webkit-user-select: none;
	-moz-user-select: none;
	-ms-user-select: none;
	user-select: none;

}

	.line-numbers-rows > span {
		pointer-events: none;
		display: block;
		counter-increment: linenumber;
	}

		.line-numbers-rows > span:before {
			content: counter(linenumber);
			color: #999;
			display: block;
			padding-right: 0.8em;
			text-align: right;
		}
</style>


</head>

<body>

<h1 id="toc_0">awesome-free-deep-learning-papers</h1>

<h3 id="toc_1">Survey Review</h3>

<ul>
<li>Deep learning (2015), Y. LeCun, Y. Bengio and G. Hinton <a href="https://www.cs.toronto.edu/%7Ehinton/absps/NatureDeepReview.pdf">[pdf]</a> :sparkles:</li>
<li>Deep learning in neural networks: An overview (2015), J. Schmidhuber <a href="http://www2.econ.iastate.edu/tesfatsi/DeepLearningInNeuralNetworksOverview.JSchmidhuber2015.pdf">[pdf]</a> :sparkles:</li>
<li>Representation learning: A review and new perspectives (2013), Y. Bengio et al. <a href="http://www.cl.uni-heidelberg.de/courses/ws14/deepl/BengioETAL12.pdf">[pdf]</a> :sparkles:</li>
</ul>

<h3 id="toc_2">Theory Future</h3>

<ul>
<li>Distilling the knowledge in a neural network (2015), G. Hinton et al. <a href="http://arxiv.org/pdf/1503.02531">[pdf]</a></li>
<li>Deep neural networks are easily fooled: High confidence predictions for unrecognizable images (2015), A. Nguyen et al. <a href="http://arxiv.org/pdf/1412.1897">[pdf]</a></li>
<li>How transferable are features in deep neural networks? (2014), J. Yosinski et al. <em>(Bengio)</em> <a href="http://papers.nips.cc/paper/5347-how-transferable-are-features-in-deep-neural-networks.pdf">[pdf]</a></li>
<li>Why does unsupervised pre-training help deep learning (2010), E. Erhan et al. <em>(Bengio)</em> <a href="http://machinelearning.wustl.edu/mlpapers/paper_files/AISTATS2010_ErhanCBV10.pdf">[pdf]</a></li>
<li>Understanding the difficulty of training deep feedforward neural networks (2010), X. Glorot and Y. Bengio <a href="http://machinelearning.wustl.edu/mlpapers/paper_files/AISTATS2010_GlorotB10.pdf">[pdf]</a></li>
</ul>

<h3 id="toc_3">Optimization Regularization</h3>

<ul>
<li>Taking the human out of the loop: A review of bayesian optimization (2016), B. Shahriari et al. <a href="https://www.cs.ox.ac.uk/people/nando.defreitas/publications/BayesOptLoop.pdf">[pdf]</a></li>
<li>Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift (2015), S. Loffe and C. Szegedy <a href="http://arxiv.org/pdf/1502.03167">[pdf]</a> :sparkles:</li>
<li>Delving deep into rectifiers: Surpassing human-level performance on imagenet classification (2015), K. He et al. <a href="http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/He_Delving_Deep_into_ICCV_2015_paper.pdf">[pdf]</a> :sparkles:</li>
<li>Dropout: A simple way to prevent neural networks from overfitting (2014), N. Srivastava et al. <em>(Hinton)</em> <a href="http://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf">[pdf]</a> :sparkles:</li>
<li>Adam: A method for stochastic optimization (2014), D. Kingma and J. Ba <a href="http://arxiv.org/pdf/1412.6980">[pdf]</a></li>
<li>Regularization of neural networks using dropconnect (2013), L. Wan et al. <em>(LeCun)</em> <a href="http://machinelearning.wustl.edu/mlpapers/paper_files/icml2013_wan13.pdf">[pdf]</a></li>
<li>Improving neural networks by preventing co-adaptation of feature detectors (2012), G. Hinton et al. <a href="http://arxiv.org/pdf/1207.0580.pdf">[pdf]</a> :sparkles:</li>
<li>Spatial pyramid pooling in deep convolutional networks for visual recognition (2014), K. He et al. <a href="http://arxiv.org/pdf/1406.4729">[pdf]</a></li>
<li>Random search for hyper-parameter optimization (2012) J. Bergstra and Y. Bengio <a href="http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a">[pdf]</a></li>
</ul>

<h3 id="toc_4">NetworkModels</h3>

<ul>
<li>Deep residual learning for image recognition (2016), K. He et al. <em>(Microsoft)</em> <a href="http://arxiv.org/pdf/1512.03385">[pdf]</a> :sparkles:</li>
<li>Going deeper with convolutions (2015), C. Szegedy et al. <em>(Google)</em> <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf">[pdf]</a> :sparkles:</li>
<li>Fast R-CNN (2015), R. Girshick <a href="http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Girshick_Fast_R-CNN_ICCV_2015_paper.pdf">[pdf]</a> :sparkles:</li>
<li>Very deep convolutional networks for large-scale image recognition (2014), K. Simonyan and A. Zisserman <a href="http://arxiv.org/pdf/1409.1556">[pdf]</a> :sparkles:</li>
<li>Fully convolutional networks for semantic segmentation (2015), J. Long et al. <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Long_Fully_Convolutional_Networks_2015_CVPR_paper.pdf">[pdf]</a> :sparkles:</li>
<li>OverFeat: Integrated recognition, localization and detection using convolutional networks (2014), P. Sermanet et al. <em>(LeCun)</em> <a href="http://arxiv.org/pdf/1312.6229">[pdf]</a></li>
<li>Visualizing and understanding convolutional networks (2014), M. Zeiler and R. Fergus <a href="http://arxiv.org/pdf/1311.2901">[pdf]</a> :sparkles:</li>
<li>Maxout networks (2013), I. Goodfellow et al. <em>(Bengio)</em> <a href="http://arxiv.org/pdf/1302.4389v4">[pdf]</a></li>
<li>ImageNet classification with deep convolutional neural networks (2012), A. Krizhevsky et al. <em>(Hinton)</em> <a href="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf">[pdf]</a> :sparkles:</li>
<li>Large scale distributed deep networks (2012), J. Dean et al. <a href="http://papers.nips.cc/paper/4687-large-scale-distributed-deep-networks.pdf">[pdf]</a> :sparkles:</li>
<li>Deep sparse rectifier neural networks (2011), X. Glorot et al. <em>(Bengio)</em> <a href="http://machinelearning.wustl.edu/mlpapers/paper_files/AISTATS2011_GlorotBB11.pdf">[pdf]</a></li>
</ul>

<h3 id="toc_5">Image</h3>

<ul>
<li>Imagenet large scale visual recognition challenge (2015), O. Russakovsky et al. <a href="http://arxiv.org/pdf/1409.0575">[pdf]</a> :sparkles:</li>
<li>Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks (2015), S. Ren et al. <a href="http://papers.nips.cc/paper/5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks.pdf">[pdf]</a></li>
<li>DRAW: A recurrent neural network for image generation (2015), K. Gregor et al. <a href="http://arxiv.org/pdf/1502.04623">[pdf]</a></li>
<li>Rich feature hierarchies for accurate object detection and semantic segmentation (2014), R. Girshick et al. <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.pdf">[pdf]</a></li>
<li>Learning and transferring mid-Level image representations using convolutional neural networks (2014), M. Oquab et al. <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Oquab_Learning_and_Transferring_2014_CVPR_paper.pdf">[pdf]</a></li>
<li>DeepFace: Closing the Gap to Human-Level Performance in Face Verification (2014), Y. Taigman et al. <em>(Facebook)</em> <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Taigman_DeepFace_Closing_the_2014_CVPR_paper.pdf">[pdf]</a></li>
<li>Decaf: A deep convolutional activation feature for generic visual recognition (2013), J. Donahue et al. <a href="http://arxiv.org/pdf/1310.1531">[pdf]</a></li>
<li>Learning Hierarchical Features for Scene Labeling (2013), C. Farabet et al. <em>(LeCun)</em> <a href="https://hal-enpc.archives-ouvertes.fr/docs/00/74/20/77/PDF/farabet-pami-13.pdf">[pdf]</a></li>
<li>Learning hierarchical invariant spatio-temporal features for action recognition with independent subspace analysis (2011), Q. Le et al. <a href="http://robotics.stanford.edu/%7Ewzou/cvpr_LeZouYeungNg11.pdf">[pdf]</a></li>
<li>Learning mid-level features for recognition (2010), Y. Boureau <em>(LeCun)</em> <a href="http://ece.duke.edu/%7Elcarin/boureau-cvpr-10.pdf">[pdf]</a></li>
</ul>

<h3 id="toc_6">Caption</h3>

<ul>
<li>Show, attend and tell: Neural image caption generation with visual attention (2015), K. Xu et al. <em>(Bengio)</em> <a href="http://arxiv.org/pdf/1502.03044">[pdf]</a> :sparkles:</li>
<li>Show and tell: A neural image caption generator (2015), O. Vinyals et al. <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Vinyals_Show_and_Tell_2015_CVPR_paper.pdf">[pdf]</a> :sparkles:</li>
<li>Long-term recurrent convolutional networks for visual recognition and description (2015), J. Donahue et al. <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.pdf">[pdf]</a> :sparkles:</li>
<li>Deep visual-semantic alignments for generating image descriptions (2015), A. Karpathy and L. Fei-Fei <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Karpathy_Deep_Visual-Semantic_Alignments_2015_CVPR_paper.html">[pdf]</a> :sparkles:</li>
</ul>

<h3 id="toc_7">Video HumanActivity</h3>

<ul>
<li>Large-scale video classification with convolutional neural networks (2014), A. Karpathy et al. <em>(FeiFei)</em> <a href="vision.stanford.edu/pdf/karpathy14.pdf">[pdf]</a></li>
<li>A survey on human activity recognition using wearable sensors (2013), O. Lara and M. Labrador <a href="http://romisatriawahono.net/lecture/rm/survey/computer%20vision/Lara%20-%20Human%20Activity%20Recognition%20-%202013.pdf">[pdf]</a></li>
<li>3D convolutional neural networks for human action recognition (2013), S. Ji et al. <a href="http://machinelearning.wustl.edu/mlpapers/paper_files/icml2010_JiXYY10.pdf">[pdf]</a></li>
<li>Deeppose: Human pose estimation via deep neural networks (2014), A. Toshev and C. Szegedy <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Toshev_DeepPose_Human_Pose_2014_CVPR_paper.pdf">[pdf]</a></li>
<li>Action recognition with improved trajectories (2013), H. Wang and C. Schmid <a href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Wang_Action_Recognition_with_2013_ICCV_paper.pdf">[pdf]</a></li>
</ul>

<h3 id="toc_8">WordEmbedding</h3>

<ul>
<li>Glove: Global vectors for word representation (2014), J. Pennington et al. <a href="http://llcao.net/cu-deeplearning15/presentation/nn-pres.pdf">[pdf]</a> :sparkles:</li>
<li>Sequence to sequence learning with neural networks (2014), I. Sutskever et al. <a href="http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf">[pdf]</a></li>
<li>Distributed representations of sentences and documents (2014), Q. Le and T. Mikolov <a href="http://arxiv.org/pdf/1405.4053">[pdf]</a> <em>(Google)</em> :sparkles:</li>
<li>Distributed representations of words and phrases and their compositionality (2013), T. Mikolov et al. <em>(Google)</em> <a href="http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf">[pdf]</a> :sparkles:</li>
<li>Efficient estimation of word representations in vector space (2013), T. Mikolov et al. <em>(Google)</em> <a href="http://arxiv.org/pdf/1301.3781">[pdf]</a> :sparkles:</li>
<li>Word representations: a simple and general method for semi-supervised learning (2010), J. Turian <em>(Bengio)</em> <a href="http://www.anthology.aclweb.org/P/P10/P10-1040.pdf">[pdf]</a></li>
</ul>

<h3 id="toc_9">MachineTranslation QnA</h3>

<ul>
<li>Towards ai-complete question answering: A set of prerequisite toy tasks (2015), J. Weston et al. <a href="http://arxiv.org/pdf/1502.05698">[pdf]</a></li>
<li>Neural machine translation by jointly learning to align and translate (2014), D. Bahdanau et al. <em>(Bengio)</em> <a href="http://arxiv.org/pdf/1409.0473">[pdf]</a> :sparkles:</li>
<li>Learning phrase representations using RNN encoder-decoder for statistical machine translation (2014), K. Cho et al. <em>(Bengio)</em> <a href="http://arxiv.org/pdf/1406.1078">[pdf]</a></li>
<li>A convolutional neural network for modelling sentences (2014), N. kalchbrenner et al. <a href="http://arxiv.org/pdf/1404.2188v1">[pdf]</a></li>
<li>Convolutional neural networks for sentence classification (2014), Y. Kim <a href="http://arxiv.org/pdf/1408.5882">[pdf]</a></li>
<li>The stanford coreNLP natural language processing toolkit (2014), C. Manning et al. <a href="http://www.surdeanu.info/mihai/papers/acl2014-corenlp.pdf">[pdf]</a></li>
<li>Recursive deep models for semantic compositionality over a sentiment treebank (2013), R. Socher et al. <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.383.1327&amp;rep=rep1&amp;type=pdf">[pdf]</a> :sparkles:</li>
<li>Natural language processing (almost) from scratch (2011), R. Collobert et al. <a href="http://arxiv.org/pdf/1103.0398">[pdf]</a></li>
<li>Recurrent neural network based language model (2010), T. Mikolov et al. <a href="http://www.fit.vutbr.cz/research/groups/speech/servite/2010/rnnlm_mikolov.pdf">[pdf]</a></li>
</ul>

<h3 id="toc_10">Speech Etc.</h3>

<ul>
<li>Speech recognition with deep recurrent neural networks (2013), A. Graves <em>(Hinton)</em> <a href="http://arxiv.org/pdf/1303.5778.pdf">[pdf]</a></li>
<li>Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups (2012), G. Hinton et al. <a href="http://www.cs.toronto.edu/%7Easamir/papers/SPM_DNN_12.pdf">[pdf]</a> :sparkles:</li>
<li>Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition (2012) G. Dahl et al. <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.337.7548&amp;rep=rep1&amp;type=pdf">[pdf]</a> :sparkles:</li>
</ul>

<h3 id="toc_11">RL Robotics</h3>

<ul>
<li>Mastering the game of Go with deep neural networks and tree search, D. Silver et al. <em>(DeepMind)</em> <a href="Mastering%20the%20game%20of%20Go%20with%20deep%20neural%20networks%20and%20tree%20search">[pdf]</a></li>
<li>Human-level control through deep reinforcement learning (2015), V. Mnih et al. <em>(DeepMind)</em> <a href="http://www.davidqiu.com:8888/research/nature14236.pdf">[pdf]</a> :sparkles:</li>
<li>Deep learning for detecting robotic grasps (2015), I. Lenz et al. <a href="http://www.cs.cornell.edu/%7Easaxena/papers/lenz_lee_saxena_deep_learning_grasping_ijrr2014.pdf">[pdf]</a></li>
<li>Playing atari with deep reinforcement learning (2013), V. Mnih et al. <em>(DeepMind)</em> <a href="http://arxiv.org/pdf/1312.5602.pdf">[pdf]</a>)</li>
</ul>

<h3 id="toc_12">Unsupervised</h3>

<ul>
<li>Building high-level features using large scale unsupervised learning (2013), Q. Le et al. <a href="http://arxiv.org/pdf/1112.6209">[pdf]</a> :sparkles:</li>
<li>Contractive auto-encoders: Explicit invariance during feature extraction (2011), S. Rifai et al. <em>(Bengio)</em> <a href="http://machinelearning.wustl.edu/mlpapers/paper_files/ICML2011Rifai_455.pdf">[pdf]</a></li>
<li>An analysis of single-layer networks in unsupervised feature learning (2011), A. Coates et al. <a href="http://machinelearning.wustl.edu/mlpapers/paper_files/AISTATS2011_CoatesNL11.pdf">[pdf]</a></li>
<li>Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion (2010), P. Vincent et al. <em>(Bengio)</em> <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.297.3484&amp;rep=rep1&amp;type=pdf">[pdf]</a></li>
<li>A practical guide to training restricted boltzmann machines (2010), G. Hinton <a href="http://www.csri.utoronto.ca/%7Ehinton/absps/guideTR.pdf">[pdf]</a></li>
<li>Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion (2010), P. Vincent et al. <em>(Bengio)</em> <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.297.3484&amp;rep=rep1&amp;type=pdf">[pdf]</a></li>
</ul>

<h3 id="toc_13">Hardware Software</h3>

<ul>
<li>TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems (2016), M. Abadi et al. <em>(Google)</em> <a href="http://arxiv.org/pdf/1603.04467">[pdf]</a></li>
<li>MatConvNet: Convolutional neural networks for matlab (2015), A. Vedaldi and K. Lenc <a href="http://arxiv.org/pdf/1412.4564">[pdf]</a></li>
<li>Caffe: Convolutional architecture for fast feature embedding (2014), Y. Jia et al. <a href="http://arxiv.org/pdf/1408.5093">[pdf]</a> :sparkles:</li>
<li>Theano: new features and speed improvements (2012), F. Bastien et al. <em>(Bengio)</em> <a href="http://arxiv.org/pdf/1211.5590">[pdf]</a></li>
</ul>

<h2 id="toc_14">License</h2>

<p><a href="https://creativecommons.org/publicdomain/zero/1.0/"><img src="http://mirrors.creativecommons.org/presskit/buttons/88x31/svg/cc-zero.svg" alt="CC0"></a></p>



<script type="text/javascript">
var _self="undefined"!=typeof window?window:"undefined"!=typeof WorkerGlobalScope&&self instanceof WorkerGlobalScope?self:{},Prism=function(){var e=/\blang(?:uage)?-(\w+)\b/i,t=0,n=_self.Prism={util:{encode:function(e){return e instanceof a?new a(e.type,n.util.encode(e.content),e.alias):"Array"===n.util.type(e)?e.map(n.util.encode):e.replace(/&/g,"&amp;").replace(/</g,"&lt;").replace(/\u00a0/g," ")},type:function(e){return Object.prototype.toString.call(e).match(/\[object (\w+)\]/)[1]},objId:function(e){return e.__id||Object.defineProperty(e,"__id",{value:++t}),e.__id},clone:function(e){var t=n.util.type(e);switch(t){case"Object":var a={};for(var r in e)e.hasOwnProperty(r)&&(a[r]=n.util.clone(e[r]));return a;case"Array":return e.map&&e.map(function(e){return n.util.clone(e)})}return e}},languages:{extend:function(e,t){var a=n.util.clone(n.languages[e]);for(var r in t)a[r]=t[r];return a},insertBefore:function(e,t,a,r){r=r||n.languages;var l=r[e];if(2==arguments.length){a=arguments[1];for(var i in a)a.hasOwnProperty(i)&&(l[i]=a[i]);return l}var o={};for(var s in l)if(l.hasOwnProperty(s)){if(s==t)for(var i in a)a.hasOwnProperty(i)&&(o[i]=a[i]);o[s]=l[s]}return n.languages.DFS(n.languages,function(t,n){n===r[e]&&t!=e&&(this[t]=o)}),r[e]=o},DFS:function(e,t,a,r){r=r||{};for(var l in e)e.hasOwnProperty(l)&&(t.call(e,l,e[l],a||l),"Object"!==n.util.type(e[l])||r[n.util.objId(e[l])]?"Array"!==n.util.type(e[l])||r[n.util.objId(e[l])]||(r[n.util.objId(e[l])]=!0,n.languages.DFS(e[l],t,l,r)):(r[n.util.objId(e[l])]=!0,n.languages.DFS(e[l],t,null,r)))}},plugins:{},highlightAll:function(e,t){var a={callback:t,selector:'code[class*="language-"], [class*="language-"] code, code[class*="lang-"], [class*="lang-"] code'};n.hooks.run("before-highlightall",a);for(var r,l=a.elements||document.querySelectorAll(a.selector),i=0;r=l[i++];)n.highlightElement(r,e===!0,a.callback)},highlightElement:function(t,a,r){for(var l,i,o=t;o&&!e.test(o.className);)o=o.parentNode;o&&(l=(o.className.match(e)||[,""])[1],i=n.languages[l]),t.className=t.className.replace(e,"").replace(/\s+/g," ")+" language-"+l,o=t.parentNode,/pre/i.test(o.nodeName)&&(o.className=o.className.replace(e,"").replace(/\s+/g," ")+" language-"+l);var s=t.textContent,u={element:t,language:l,grammar:i,code:s};if(!s||!i)return n.hooks.run("complete",u),void 0;if(n.hooks.run("before-highlight",u),a&&_self.Worker){var c=new Worker(n.filename);c.onmessage=function(e){u.highlightedCode=e.data,n.hooks.run("before-insert",u),u.element.innerHTML=u.highlightedCode,r&&r.call(u.element),n.hooks.run("after-highlight",u),n.hooks.run("complete",u)},c.postMessage(JSON.stringify({language:u.language,code:u.code,immediateClose:!0}))}else u.highlightedCode=n.highlight(u.code,u.grammar,u.language),n.hooks.run("before-insert",u),u.element.innerHTML=u.highlightedCode,r&&r.call(t),n.hooks.run("after-highlight",u),n.hooks.run("complete",u)},highlight:function(e,t,r){var l=n.tokenize(e,t);return a.stringify(n.util.encode(l),r)},tokenize:function(e,t){var a=n.Token,r=[e],l=t.rest;if(l){for(var i in l)t[i]=l[i];delete t.rest}e:for(var i in t)if(t.hasOwnProperty(i)&&t[i]){var o=t[i];o="Array"===n.util.type(o)?o:[o];for(var s=0;s<o.length;++s){var u=o[s],c=u.inside,g=!!u.lookbehind,h=!!u.greedy,f=0,d=u.alias;u=u.pattern||u;for(var p=0;p<r.length;p++){var m=r[p];if(r.length>e.length)break e;if(!(m instanceof a)){u.lastIndex=0;var y=u.exec(m),v=1;if(!y&&h&&p!=r.length-1){var b=r[p+1].matchedStr||r[p+1],k=m+b;if(p<r.length-2&&(k+=r[p+2].matchedStr||r[p+2]),u.lastIndex=0,y=u.exec(k),!y)continue;var w=y.index+(g?y[1].length:0);if(w>=m.length)continue;var _=y.index+y[0].length,P=m.length+b.length;if(v=3,P>=_){if(r[p+1].greedy)continue;v=2,k=k.slice(0,P)}m=k}if(y){g&&(f=y[1].length);var w=y.index+f,y=y[0].slice(f),_=w+y.length,S=m.slice(0,w),O=m.slice(_),j=[p,v];S&&j.push(S);var A=new a(i,c?n.tokenize(y,c):y,d,y,h);j.push(A),O&&j.push(O),Array.prototype.splice.apply(r,j)}}}}}return r},hooks:{all:{},add:function(e,t){var a=n.hooks.all;a[e]=a[e]||[],a[e].push(t)},run:function(e,t){var a=n.hooks.all[e];if(a&&a.length)for(var r,l=0;r=a[l++];)r(t)}}},a=n.Token=function(e,t,n,a,r){this.type=e,this.content=t,this.alias=n,this.matchedStr=a||null,this.greedy=!!r};if(a.stringify=function(e,t,r){if("string"==typeof e)return e;if("Array"===n.util.type(e))return e.map(function(n){return a.stringify(n,t,e)}).join("");var l={type:e.type,content:a.stringify(e.content,t,r),tag:"span",classes:["token",e.type],attributes:{},language:t,parent:r};if("comment"==l.type&&(l.attributes.spellcheck="true"),e.alias){var i="Array"===n.util.type(e.alias)?e.alias:[e.alias];Array.prototype.push.apply(l.classes,i)}n.hooks.run("wrap",l);var o="";for(var s in l.attributes)o+=(o?" ":"")+s+'="'+(l.attributes[s]||"")+'"';return"<"+l.tag+' class="'+l.classes.join(" ")+'" '+o+">"+l.content+"</"+l.tag+">"},!_self.document)return _self.addEventListener?(_self.addEventListener("message",function(e){var t=JSON.parse(e.data),a=t.language,r=t.code,l=t.immediateClose;_self.postMessage(n.highlight(r,n.languages[a],a)),l&&_self.close()},!1),_self.Prism):_self.Prism;var r=document.currentScript||[].slice.call(document.getElementsByTagName("script")).pop();return r&&(n.filename=r.src,document.addEventListener&&!r.hasAttribute("data-manual")&&document.addEventListener("DOMContentLoaded",n.highlightAll)),_self.Prism}();"undefined"!=typeof module&&module.exports&&(module.exports=Prism),"undefined"!=typeof global&&(global.Prism=Prism);
</script>

<script type="text/javascript">
!function(){"undefined"!=typeof self&&self.Prism&&self.document&&Prism.hooks.add("complete",function(e){if(e.code){var t=e.element.parentNode,s=/\s*\bline-numbers\b\s*/;if(t&&/pre/i.test(t.nodeName)&&(s.test(t.className)||s.test(e.element.className))&&!e.element.querySelector(".line-numbers-rows")){s.test(e.element.className)&&(e.element.className=e.element.className.replace(s,"")),s.test(t.className)||(t.className+=" line-numbers");var n,a=e.code.match(/\n(?!$)/g),l=a?a.length+1:1,m=new Array(l+1);m=m.join("<span></span>"),n=document.createElement("span"),n.className="line-numbers-rows",n.innerHTML=m,t.hasAttribute("data-start")&&(t.style.counterReset="linenumber "+(parseInt(t.getAttribute("data-start"),10)-1)),e.element.appendChild(n)}}})}();
</script>

<script type="text/x-mathjax-config">
if (typeof MathJaxListener !== 'undefined') {
  MathJax.Hub.Register.StartupHook('End', function () {
    MathJaxListener.invokeCallbackForKey_('End');
  });
}
</script>

<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


</body>

</html>
